{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PureMic Dataset Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve,average_precision_score, precision_recall_curve, auc, make_scorer, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "DATA_ROOT='D:\\CastelBranco\\PAPER'\n",
    "all_files=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "The puremic data is provided in a python-friendly format just like openmic-2018:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUREMIC = np.load(os.path.join(DATA_ROOT, 'PureMic.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X', 'Y', 'sample_key']\n"
     ]
    }
   ],
   "source": [
    "print(list(PUREMIC.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's included in the data?\n",
    "- X: 1050 91 128 array of VGGish features\n",
    "    - First index (0..1049) corresponds to the sample key\n",
    "    - Second index (0..90) corresponds to the 91, 100 ms hop frames (each time slice is 960 ms long). \n",
    "    - Third index (0..127) corresponds to the VGGish features at each point in the 10sec clip\n",
    "    - Example X[40, 8] is the 128-dimensional feature vector for the 9th time slice in the 41st example\n",
    "\n",
    "- Y_true: 1050 21 one hot encoded label array\n",
    "    - First index corresponds to sample key, as above\n",
    "    - Second index corresponds to the label class (accordion, ..., zilence)\n",
    "    - Example: Y[40, 4] indicates the the label of the 5th instrument for example #41\n",
    "\n",
    "- sample_key: 1050 array of sample key strings\n",
    "    - Example: sample_key[40] is the sample key for example #41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_PureMic, Y_PureMic, sample_key_PureMic = PUREMIC['X'], PUREMIC['Y'], PUREMIC['sample_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accordion': 0, 'banjo': 1, 'bass': 2, 'cello': 3, 'clarinet': 4, 'cymbals': 5, 'drums': 6, 'flute': 7, 'guitar': 8, 'mallet_percussion': 9, 'mandolin': 10, 'organ': 11, 'piano': 12, 'saxophone': 13, 'synthesizer': 14, 'trombone': 15, 'trumpet': 16, 'ukulele': 17, 'violin': 18, 'voice': 19, 'zilence': 20}\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(DATA_ROOT, 'class-map.json'), 'r') as f:\n",
    "    class_map = json.load(f)\n",
    "\n",
    "#%%\n",
    "classes=[]\n",
    "for value in class_map:\n",
    "    classes.append(value)\n",
    "    \n",
    "classes=np.array(classes)\n",
    "print(class_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading train and test splits\n",
    "PureMic also provides a pre-defined train-test split. The sets are perfectly balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_train = pd.read_csv(os.path.join(DATA_ROOT,'train_split.csv'), \n",
    "                          header=None, squeeze=True)\n",
    "\n",
    "split_test = pd.read_csv(os.path.join(DATA_ROOT,'test_split.csv'), \n",
    "                          header=None, squeeze=True)\n",
    "\n",
    "train_set = set(split_train)\n",
    "test_set = set(split_test)\n",
    "\n",
    "\n",
    "idx_train, idx_test = [], []\n",
    "\n",
    "for idx, n in enumerate(sample_key_PureMic):\n",
    "    if n in train_set:\n",
    "        idx_train.append(idx)\n",
    "    elif n in test_set:\n",
    "        idx_test.append(idx)\n",
    "    else:\n",
    "        # This should never happen, but better safe than sorry.\n",
    "        raise RuntimeError('Unknown sample key={}! Abort!'.format(sample_key_PureMic[n]))\n",
    "        \n",
    "# Finally, cast the idx_* arrays to numpy structures\n",
    "idx_train = np.asarray(idx_train)\n",
    "idx_test = np.asarray(idx_test)\n",
    "\n",
    "\n",
    "# Finally, we use the split indices to partition the features, labels, and masks\n",
    "X_train_pm = X_PureMic[idx_train]\n",
    "X_test_pm = X_PureMic[idx_test]\n",
    "\n",
    "Y_train_pm = Y_PureMic[idx_train]\n",
    "Y_test_pm = Y_PureMic[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train: 840,  # Test: 210\n"
     ]
    }
   ],
   "source": [
    "# Number of train and test examples (80%/20%)\n",
    "print('# Train: {},  # Test: {}'.format(len(split_train), len(split_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(840, 91, 128)\n",
      "(210, 91, 128)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_pm.shape)\n",
    "print(X_test_pm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_PM_MEAN_TRAIN = np.mean(X_train_pm, axis=1)\n",
    "X_PM_MEAN_TEST = np.mean(X_test_pm, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "        accordion       1.00      1.00      1.00        10\n",
      "            banjo       1.00      1.00      1.00        10\n",
      "             bass       1.00      1.00      1.00        10\n",
      "            cello       1.00      0.90      0.95        10\n",
      "         clarinet       1.00      0.90      0.95        10\n",
      "          cymbals       1.00      0.70      0.82        10\n",
      "            drums       0.80      0.80      0.80        10\n",
      "            flute       1.00      1.00      1.00        10\n",
      "           guitar       0.91      1.00      0.95        10\n",
      "mallet_percussion       0.91      1.00      0.95        10\n",
      "         mandolin       1.00      0.90      0.95        10\n",
      "            organ       1.00      1.00      1.00        10\n",
      "            piano       1.00      0.90      0.95        10\n",
      "        saxophone       0.90      0.90      0.90        10\n",
      "      synthesizer       0.91      1.00      0.95        10\n",
      "         trombone       1.00      1.00      1.00        10\n",
      "          trumpet       1.00      0.90      0.95        10\n",
      "          ukulele       0.91      1.00      0.95        10\n",
      "           violin       0.83      1.00      0.91        10\n",
      "            voice       0.77      1.00      0.87        10\n",
      "          zilence       1.00      0.90      0.95        10\n",
      "\n",
      "        micro avg       0.94      0.94      0.94       210\n",
      "        macro avg       0.95      0.94      0.94       210\n",
      "     weighted avg       0.95      0.94      0.94       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#FIT NN_MEAN \n",
    "tf.keras.backend.clear_session()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "num_classes=21\n",
    "img_input = Input(shape=(128,))\n",
    "n=Dense(512, activation='sigmoid')(img_input)\n",
    "n=Dense(256, activation='sigmoid')(n)\n",
    "n=Dense(num_classes, activation='softmax')(n)\n",
    "NN_MEAN=Model(img_input,n)\n",
    "\n",
    "opt= SGD(lr=0.001)\n",
    "NN_MEAN.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', \n",
    "                        min_delta=1e-3, \n",
    "                        patience=8, verbose=2, mode='auto')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, \n",
    "                                            verbose=0, mode='auto', \n",
    "                                            min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "if all_files:\n",
    "    NN_MEAN.load_weights(os.path.join(DATA_ROOT,'models','NN_MEAN.h5'))\n",
    "else:\n",
    "    NN_MEAN.fit(X_PM_MEAN_TRAIN,Y_train_pm,validation_data=(X_PM_MEAN_TEST,Y_test_pm),callbacks=[monitor,reduce_lr],verbose=2,epochs=2000)\n",
    "    NN_MEAN.save_weights(os.path.join(DATA_ROOT,'models','NN_MEAN.h5'))    \n",
    "\n",
    "scores_pm_mean=NN_MEAN.predict_on_batch(X_PM_MEAN_TEST)\n",
    "\n",
    "print(classification_report(np.argmax(Y_test_pm,axis=1), np.argmax(scores_pm_mean,axis=1),target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of PM_ALL: 95550\n"
     ]
    }
   ],
   "source": [
    "#%% Create PM_ALL\n",
    "\n",
    "X_PM_ALL=[]\n",
    "Y_PM_ALL=[]\n",
    "samp_info_all_pm=[]\n",
    "\n",
    "for idx, clip in enumerate(X_PureMic):\n",
    "    for sec, instance in enumerate(clip):\n",
    "        X_PM_ALL.append(instance)\n",
    "        Y_PM_ALL.append(Y_PureMic[idx])\n",
    "        samp_info_all_pm.append([sample_key_PureMic[idx],sec])\n",
    "            \n",
    "X_PM_ALL=np.array(X_PM_ALL)\n",
    "Y_PM_ALL=np.array(Y_PM_ALL)\n",
    "samp_info_all_pm=np.array(samp_info_all_pm)\n",
    "\n",
    "print('Size of PM_ALL: {}'.format(len(X_PM_ALL)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'accordion': 4550,\n",
       "         'banjo': 4550,\n",
       "         'bass': 4550,\n",
       "         'cello': 4550,\n",
       "         'clarinet': 4550,\n",
       "         'cymbals': 4550,\n",
       "         'drums': 4550,\n",
       "         'flute': 4550,\n",
       "         'guitar': 4550,\n",
       "         'mallet_percussion': 4550,\n",
       "         'mandolin': 4550,\n",
       "         'organ': 4550,\n",
       "         'piano': 4550,\n",
       "         'saxophone': 4550,\n",
       "         'synthesizer': 4550,\n",
       "         'trombone': 4550,\n",
       "         'trumpet': 4550,\n",
       "         'ukulele': 4550,\n",
       "         'violin': 4550,\n",
       "         'voice': 4550,\n",
       "         'zilence': 4550})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%% convert one hot enconded to categorical so we can see the class distribution\n",
    "pm_labels=[]\n",
    "\n",
    "for n in Y_PM_ALL:\n",
    "    inst_class=classes[np.where(n==1)[0][0]]\n",
    "    pm_labels.append(inst_class)\n",
    "    \n",
    "collections.Counter(pm_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get PM_ALL scores with NN_MEAN\n",
    "scores_pm_all=NN_MEAN.predict(X_PM_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of PM1: 84020\n"
     ]
    }
   ],
   "source": [
    "#create PM_1\n",
    "thresh_sil=0.5\n",
    "thresh_up=0.2\n",
    "\n",
    "sil_label=np.zeros(21,dtype=np.int32)\n",
    "sil_label[20]=1\n",
    "\n",
    "X_PM1=[]\n",
    "Y_PM1=[]\n",
    "N_PM1=[]\n",
    "\n",
    "for idx in range(len(X_PM_ALL)):    \n",
    "    \n",
    "    if scores_pm_all[idx,20]>thresh_sil: #look for silence instances in all classes\n",
    "        X_PM1.append(X_PM_ALL[idx])\n",
    "        Y_PM1.append(sil_label)\n",
    "        N_PM1.append(samp_info_all_pm[idx])\n",
    "    \n",
    "    else:\n",
    "        classe=np.where(Y_PM_ALL[idx]==1)[0][0] #get the class index of current instance\n",
    "        proba=scores_pm_all[idx,classe] #get the score only of the corresponding class\n",
    "        if proba > thresh_up:           #even belonging to same classe the activation should be greater than thresh_up\n",
    "            X_PM1.append(X_PM_ALL[idx])\n",
    "            Y_PM1.append(Y_PM_ALL[idx])\n",
    "            N_PM1.append(samp_info_all_pm[idx])\n",
    "        \n",
    "\n",
    "X_PM1=np.array(X_PM1)\n",
    "Y_PM1=np.array(Y_PM1)\n",
    "N_PM1=np.array(N_PM1)\n",
    "\n",
    "print('Size of PM1: {}'.format(len(X_PM1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'accordion': 4273,\n",
       "         'zilence': 4809,\n",
       "         'banjo': 4126,\n",
       "         'bass': 4020,\n",
       "         'cello': 3673,\n",
       "         'clarinet': 3691,\n",
       "         'cymbals': 3781,\n",
       "         'drums': 4156,\n",
       "         'flute': 4145,\n",
       "         'guitar': 4052,\n",
       "         'mallet_percussion': 4036,\n",
       "         'mandolin': 4188,\n",
       "         'organ': 3995,\n",
       "         'piano': 4053,\n",
       "         'saxophone': 3273,\n",
       "         'synthesizer': 3945,\n",
       "         'trombone': 3901,\n",
       "         'trumpet': 3683,\n",
       "         'ukulele': 3932,\n",
       "         'violin': 3981,\n",
       "         'voice': 4307})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert one hot enconded to categorical so we can see the class distribution\n",
    "#this time, the number of instances should increase for silence class and decrease to the remaining\n",
    "pm1_labels=[]\n",
    "\n",
    "for n in Y_PM1:\n",
    "    inst_class=classes[np.where(n==1)[0][0]]\n",
    "    pm1_labels.append(inst_class)\n",
    "    \n",
    "collections.Counter(pm1_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split again in train-test, this time with labels per instance and not per clip\n",
    "idx_train, idx_test = [], []\n",
    "\n",
    "for idx, n in enumerate(N_PM1):\n",
    "    if n[0] in train_set:\n",
    "        idx_train.append(idx)\n",
    "    elif n[0] in test_set:\n",
    "        idx_test.append(idx)\n",
    "    else:\n",
    "        raise RuntimeError('Unknown sample key={}! Abort!'.format(sample_key_PureMic[n]))\n",
    "        \n",
    "idx_train_pm1 = np.asarray(idx_train)\n",
    "idx_test_pm1 = np.asarray(idx_test)\n",
    "\n",
    "X_PM1_TRAIN=X_PM1[idx_train_pm1]\n",
    "Y_PM1_TRAIN=Y_PM1[idx_train_pm1]\n",
    "\n",
    "X_PM1_TEST=X_PM1[idx_test_pm1]\n",
    "Y_PM1_TEST=Y_PM1[idx_test_pm1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train: 68139,  # Test: 15881\n"
     ]
    }
   ],
   "source": [
    "# Number of train and test examples according to the same division\n",
    "print('# Train: {},  # Test: {}'.format(len(X_PM1_TRAIN), len(X_PM1_TEST)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "        accordion       0.96      0.99      0.98       887\n",
      "            banjo       1.00      0.99      1.00       799\n",
      "             bass       0.96      0.97      0.96       793\n",
      "            cello       0.94      0.94      0.94       682\n",
      "         clarinet       0.94      0.89      0.91       703\n",
      "          cymbals       0.98      0.93      0.96       567\n",
      "            drums       0.95      0.98      0.96       699\n",
      "            flute       0.97      0.99      0.98       870\n",
      "           guitar       0.99      0.86      0.92       735\n",
      "mallet_percussion       0.99      0.96      0.98       776\n",
      "         mandolin       0.96      0.94      0.95       822\n",
      "            organ       0.96      1.00      0.98       736\n",
      "            piano       0.96      0.98      0.97       678\n",
      "        saxophone       0.85      0.94      0.89       578\n",
      "      synthesizer       0.96      0.97      0.97       799\n",
      "         trombone       0.93      0.95      0.94       725\n",
      "          trumpet       0.90      0.91      0.90       606\n",
      "          ukulele       0.89      0.95      0.92       813\n",
      "           violin       0.97      0.88      0.92       855\n",
      "            voice       0.98      1.00      0.99       892\n",
      "          zilence       0.98      0.98      0.98       866\n",
      "\n",
      "        micro avg       0.95      0.95      0.95     15881\n",
      "        macro avg       0.95      0.95      0.95     15881\n",
      "     weighted avg       0.96      0.95      0.95     15881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "num_classes=21\n",
    "img_input = Input(shape=(128,))\n",
    "n=Dense(4096, activation='sigmoid')(img_input)\n",
    "n=Dense(2048, activation='sigmoid')(n)\n",
    "n=Dense(num_classes, activation='softmax')(n)\n",
    "NN_ALL=Model(img_input,n)\n",
    "\n",
    "opt = Adam(lr=0.001, decay=1e-6)\n",
    "opt= SGD(lr=0.001)\n",
    "NN_ALL.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', \n",
    "                        min_delta=1e-3, \n",
    "                        patience=8, verbose=2, mode='auto')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, \n",
    "                                            verbose=0, mode='auto', \n",
    "                                            min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "if all_files:\n",
    "    NN_ALL.load_weights(os.path.join(DATA_ROOT,'models','NN_ALL.h5'))\n",
    "    with open(os.path.join(DATA_ROOT,'scores','scores_pm_all'), \"rb\") as fp:   # Unpickling\n",
    "            scores_pm_all = pickle.load(fp)\n",
    "else:\n",
    "    NN_ALL.fit(X_PM1_TRAIN,Y_PM1_TRAIN,validation_data=(X_PM1_TEST,Y_PM1_TEST),callbacks=[monitor,reduce_lr],verbose=2,epochs=1000)\n",
    "    NN_ALL.save_weights(os.path.join(DATA_ROOT,'models','NN_ALL.h5'))\n",
    "    scores_pm_all=NN_ALL.predict(X_PM1_TEST)\n",
    "    with open(os.path.join(DATA_ROOT,'scores','scores_pm_all'), \"wb\") as fp:   # Unpickling\n",
    "        pickle.dump(scores_pm_all, fp,protocol=4)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "print(classification_report(np.argmax(Y_PM1_TEST,axis=1), np.argmax(scores_pm_all,axis=1),target_names=classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PM2\n",
    "PM2 is the final dataset as explained in (ref paper).\n",
    "PM2 has examples from AudioSet (without the clips of PureMic) and the training set of OpenMIC. As those dataset are very large we only realease the final result of PM2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(os.path.join(DATA_ROOT,'PM2'), \"rb\") as fp:   # Unpickling\n",
    "            X_PM2,Y_PM2 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'accordion': 50000,\n",
       "         'banjo': 50000,\n",
       "         'bass': 27536,\n",
       "         'cello': 50000,\n",
       "         'clarinet': 28355,\n",
       "         'cymbals': 35886,\n",
       "         'drums': 50000,\n",
       "         'flute': 50000,\n",
       "         'guitar': 50000,\n",
       "         'mallet_percussion': 50000,\n",
       "         'mandolin': 50000,\n",
       "         'organ': 50000,\n",
       "         'piano': 50000,\n",
       "         'saxophone': 50000,\n",
       "         'synthesizer': 50000,\n",
       "         'trombone': 50000,\n",
       "         'trumpet': 50000,\n",
       "         'ukulele': 50000,\n",
       "         'violin': 50000,\n",
       "         'voice': 50000,\n",
       "         'zilence': 50000})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of instances per class\n",
    "collections.Counter(Y_PM2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "991777"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_PM2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "        accordion       0.97      0.99      0.98       887\n",
      "            banjo       0.99      1.00      1.00       799\n",
      "             bass       0.96      0.96      0.96       793\n",
      "            cello       0.95      0.93      0.94       682\n",
      "         clarinet       0.92      0.89      0.91       703\n",
      "          cymbals       0.98      0.93      0.96       567\n",
      "            drums       0.94      0.98      0.96       699\n",
      "            flute       0.97      0.99      0.98       870\n",
      "           guitar       0.99      0.91      0.95       735\n",
      "mallet_percussion       0.99      0.98      0.99       776\n",
      "         mandolin       0.96      0.94      0.95       822\n",
      "            organ       0.97      1.00      0.99       736\n",
      "            piano       0.96      0.99      0.97       678\n",
      "        saxophone       0.87      0.91      0.89       578\n",
      "      synthesizer       0.98      0.97      0.97       799\n",
      "         trombone       0.94      0.95      0.95       725\n",
      "          trumpet       0.90      0.91      0.90       606\n",
      "          ukulele       0.89      0.96      0.92       813\n",
      "           violin       0.96      0.91      0.93       855\n",
      "            voice       0.99      1.00      0.99       892\n",
      "          zilence       0.99      0.98      0.99       866\n",
      "\n",
      "        micro avg       0.96      0.96      0.96     15881\n",
      "        macro avg       0.96      0.96      0.96     15881\n",
      "     weighted avg       0.96      0.96      0.96     15881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "PM2_y_binary = encoder.fit_transform(Y_PM2)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "num_classes=21\n",
    "img_input = Input(shape=(128,))\n",
    "n=Dense(4096, activation='sigmoid')(img_input)\n",
    "n=Dense(2048, activation='sigmoid')(n)\n",
    "n=Dense(num_classes, activation='softmax')(n)\n",
    "NN_FINAL=Model(img_input,n)\n",
    "\n",
    "opt = Adam(lr=0.001, decay=1e-6)\n",
    "opt= SGD(lr=0.001)\n",
    "NN_FINAL.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', \n",
    "                        min_delta=1e-3, \n",
    "                        patience=5, verbose=2, mode='auto')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, \n",
    "                                            verbose=0, mode='auto', \n",
    "                                            min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "if all_files:\n",
    "    NN_FINAL.load_weights(os.path.join(DATA_ROOT,'models','NN_FINAL.h5'))\n",
    "    with open(os.path.join(DATA_ROOT,'scores','scores_pm_final'), \"rb\") as fp:   # Unpickling\n",
    "            scores_pm_final = pickle.load(fp)\n",
    "    \n",
    "else:\n",
    "    NN_FINAL.fit(X_PM2,PM2_y_binary,validation_data=(X_PM1_TEST,Y_PM1_TEST),callbacks=[monitor,reduce_lr],verbose=2,epochs=1000)\n",
    "    NN_FINAL.save_weights(os.path.join(DATA_ROOT,'models','NN_FINAL.h5'))\n",
    "    scores_pm_final=NN_FINAL.predict(X_PM1_TEST)\n",
    "    with open(os.path.join(DATA_ROOT,'scores','scores_pm_final'), \"wb\") as fp:   # Unpickling\n",
    "        pickle.dump(scores_pm_final, fp,protocol=4)\n",
    "\n",
    "\n",
    "print(classification_report(np.argmax(Y_PM1_TEST,axis=1),np.argmax(scores_pm_final,axis=1),target_names=classes))            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358732 missing labels, 23654 negatives, 17614 positives\n"
     ]
    }
   ],
   "source": [
    "#Openmic missing labels\n",
    "OPENMIC = np.load(os.path.join(DATA_ROOT,'OpenMIC', 'openmic-2018.npz'))\n",
    "X_om_final, Y_true_om_final, Y_mask_om_final, sample_key_om_final = OPENMIC['X'], OPENMIC['Y_true'], OPENMIC['Y_mask'], OPENMIC['sample_key']\n",
    "\n",
    "split_train = pd.read_csv(os.path.join(DATA_ROOT, 'OpenMIC','split01_train.csv'), \n",
    "                              header=None, squeeze=True)\n",
    "split_test = pd.read_csv(os.path.join(DATA_ROOT, 'OpenMIC','split01_test.csv'), \n",
    "                     header=None, squeeze=True)\n",
    "\n",
    "train_set = set(split_train)\n",
    "test_set = set(split_test)\n",
    "\n",
    "idx_train, idx_test = [], []\n",
    "\n",
    "for idx, n in enumerate(sample_key_om_final):\n",
    "    if n in train_set:\n",
    "        idx_train.append(idx)\n",
    "    elif n in test_set:\n",
    "        idx_test.append(idx)\n",
    "\n",
    "idx_train = np.asarray(idx_train)\n",
    "idx_test = np.asarray(idx_test)\n",
    "\n",
    "X_train = X_om_final[idx_train]\n",
    "X_test = X_om_final[idx_test]\n",
    "\n",
    "unlabelled=0\n",
    "positives=0\n",
    "negatives=0\n",
    "for clip in Y_true_om_final:\n",
    "    for inst_class in clip:\n",
    "        if inst_class == 0.5:\n",
    "            unlabelled=unlabelled+1\n",
    "        if inst_class > 0.5:\n",
    "            positives=positives+1\n",
    "        if inst_class < 0.5:\n",
    "            negatives=negatives+1\n",
    "            \n",
    "\n",
    "print('{} missing labels, {} negatives, {} positives'.format(unlabelled,negatives,positives))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenMIC baseline\n",
    "The following code will process openmic baseline with the new labels. Original baseline is available at https://github.com/cosmir/openmic-2018/blob/master/examples/modeling-baseline.ipynb so you can compare the results.\n",
    "mAP (REF PAPER) of openmic baseline is **0.66**.\n",
    "\n",
    "The new proposed labels improves the openmic **original** i.e. to validate, we only changed the training set labels keeping the test set exactly the same so we can compare the results. The new mAP is **0.68** with **2165** new positive labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "accordion\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      1.00      0.98      1159\n",
      "        True       1.00      0.86      0.92       390\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1549\n",
      "   macro avg       0.98      0.93      0.95      1549\n",
      "weighted avg       0.97      0.96      0.96      1549\n",
      "\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.84      0.98      0.91       423\n",
      "        True       0.81      0.33      0.47       115\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       538\n",
      "   macro avg       0.83      0.65      0.69       538\n",
      "weighted avg       0.84      0.84      0.81       538\n",
      "\n",
      "----------------------------------------------------\n",
      "banjo\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.98      0.98      1148\n",
      "        True       0.96      0.96      0.96       607\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1755\n",
      "   macro avg       0.97      0.97      0.97      1755\n",
      "weighted avg       0.97      0.97      0.97      1755\n",
      "\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.84      0.88      0.86       338\n",
      "        True       0.67      0.59      0.63       140\n",
      "\n",
      "   micro avg       0.79      0.79      0.79       478\n",
      "   macro avg       0.75      0.73      0.74       478\n",
      "weighted avg       0.79      0.79      0.79       478\n",
      "\n",
      "----------------------------------------------------\n",
      "bass\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.98      0.98      1010\n",
      "        True       0.96      0.94      0.95       422\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1432\n",
      "   macro avg       0.97      0.96      0.96      1432\n",
      "weighted avg       0.97      0.97      0.97      1432\n",
      "\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.84      0.96      0.90       329\n",
      "        True       0.85      0.55      0.67       134\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       463\n",
      "   macro avg       0.85      0.76      0.78       463\n",
      "weighted avg       0.84      0.84      0.83       463\n",
      "\n",
      "----------------------------------------------------\n",
      "cello\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.95      0.97       866\n",
      "        True       0.94      0.99      0.96       643\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1509\n",
      "   macro avg       0.96      0.97      0.97      1509\n",
      "weighted avg       0.97      0.97      0.97      1509\n",
      "\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.80      0.84      0.82       259\n",
      "        True       0.80      0.76      0.78       226\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       485\n",
      "   macro avg       0.80      0.80      0.80       485\n",
      "weighted avg       0.80      0.80      0.80       485\n",
      "\n",
      "----------------------------------------------------\n",
      "clarinet\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.90      1.00      0.95      1349\n",
      "        True       1.00      0.65      0.79       416\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      1765\n",
      "   macro avg       0.95      0.83      0.87      1765\n",
      "weighted avg       0.93      0.92      0.91      1765\n",
      "\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.80      0.98      0.88       503\n",
      "        True       0.65      0.11      0.19       137\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       640\n",
      "   macro avg       0.73      0.55      0.54       640\n",
      "weighted avg       0.77      0.80      0.73       640\n",
      "\n",
      "----------------------------------------------------\n",
      "cymbals\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.89      0.94       485\n",
      "        True       0.94      1.00      0.97       817\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1302\n",
      "   macro avg       0.97      0.94      0.95      1302\n",
      "weighted avg       0.96      0.96      0.96      1302\n",
      "\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      0.86      0.91       139\n",
      "        True       0.94      0.98      0.96       297\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       436\n",
      "   macro avg       0.95      0.92      0.93       436\n",
      "weighted avg       0.95      0.94      0.94       436\n",
      "\n",
      "----------------------------------------------------\n",
      "drums\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.95      0.97       495\n",
      "        True       0.98      1.00      0.99      1030\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1525\n",
      "   macro avg       0.99      0.97      0.98      1525\n",
      "weighted avg       0.98      0.98      0.98      1525\n",
      "\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      0.81      0.86       146\n",
      "        True       0.91      0.96      0.93       278\n",
      "\n",
      "   micro avg       0.91      0.91      0.91       424\n",
      "   macro avg       0.91      0.89      0.90       424\n",
      "weighted avg       0.91      0.91      0.91       424\n",
      "\n",
      "----------------------------------------------------\n",
      "flute\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.99      0.98      1050\n",
      "        True       0.98      0.93      0.96       482\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1532\n",
      "   macro avg       0.97      0.96      0.97      1532\n",
      "weighted avg       0.97      0.97      0.97      1532\n",
      "\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.78      0.92      0.84       387\n",
      "        True       0.70      0.41      0.52       175\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       562\n",
      "   macro avg       0.74      0.67      0.68       562\n",
      "weighted avg       0.75      0.76      0.74       562\n",
      "\n",
      "----------------------------------------------------\n",
      "guitar\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.95      0.98       362\n",
      "        True       0.98      1.00      0.99      1015\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      1377\n",
      "   macro avg       0.99      0.98      0.98      1377\n",
      "weighted avg       0.99      0.99      0.99      1377\n",
      "\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.97      0.97       150\n",
      "        True       0.99      0.98      0.98       286\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       436\n",
      "   macro avg       0.98      0.98      0.98       436\n",
      "weighted avg       0.98      0.98      0.98       436\n",
      "\n",
      "----------------------------------------------------\n",
      "mallet_percussion\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.95      0.97       802\n",
      "        True       0.92      1.00      0.96       523\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1325\n",
      "   macro avg       0.96      0.97      0.97      1325\n",
      "weighted avg       0.97      0.97      0.97      1325\n",
      "\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.79      0.84      0.82       267\n",
      "        True       0.78      0.72      0.75       211\n",
      "\n",
      "   micro avg       0.79      0.79      0.79       478\n",
      "   macro avg       0.79      0.78      0.78       478\n",
      "weighted avg       0.79      0.79      0.79       478\n",
      "\n",
      "----------------------------------------------------\n",
      "mandolin\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.95      0.96      1185\n",
      "        True       0.93      0.95      0.94       755\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1940\n",
      "   macro avg       0.95      0.95      0.95      1940\n",
      "weighted avg       0.95      0.95      0.95      1940\n",
      "\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.84      0.81      0.83       434\n",
      "        True       0.61      0.66      0.64       193\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       627\n",
      "   macro avg       0.73      0.74      0.73       627\n",
      "weighted avg       0.77      0.77      0.77       627\n",
      "\n",
      "----------------------------------------------------\n",
      "organ\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.85      0.91       977\n",
      "        True       0.88      1.00      0.93      1067\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      2044\n",
      "   macro avg       0.94      0.92      0.92      2044\n",
      "weighted avg       0.93      0.92      0.92      2044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.90      0.69      0.78       310\n",
      "        True       0.50      0.79      0.61       121\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       431\n",
      "   macro avg       0.70      0.74      0.70       431\n",
      "weighted avg       0.78      0.72      0.73       431\n",
      "\n",
      "----------------------------------------------------\n",
      "piano\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.96      0.98       420\n",
      "        True       0.98      1.00      0.99      1017\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      1437\n",
      "   macro avg       0.99      0.98      0.99      1437\n",
      "weighted avg       0.99      0.99      0.99      1437\n",
      "\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.86      0.91       130\n",
      "        True       0.94      0.99      0.96       285\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       415\n",
      "   macro avg       0.96      0.93      0.94       415\n",
      "weighted avg       0.95      0.95      0.95       415\n",
      "\n",
      "----------------------------------------------------\n",
      "saxophone\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.93      0.96       906\n",
      "        True       0.93      0.99      0.96       831\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1737\n",
      "   macro avg       0.96      0.96      0.96      1737\n",
      "weighted avg       0.96      0.96      0.96      1737\n",
      "\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.87      0.80      0.83       324\n",
      "        True       0.81      0.87      0.84       305\n",
      "\n",
      "   micro avg       0.83      0.83      0.83       629\n",
      "   macro avg       0.84      0.84      0.83       629\n",
      "weighted avg       0.84      0.83      0.83       629\n",
      "\n",
      "----------------------------------------------------\n",
      "synthesizer\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.95      0.97       399\n",
      "        True       0.98      1.00      0.99      1099\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      1498\n",
      "   macro avg       0.99      0.97      0.98      1498\n",
      "weighted avg       0.99      0.99      0.99      1498\n",
      "\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.90      0.92       112\n",
      "        True       0.96      0.98      0.97       268\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       380\n",
      "   macro avg       0.95      0.94      0.95       380\n",
      "weighted avg       0.96      0.96      0.95       380\n",
      "\n",
      "----------------------------------------------------\n",
      "trombone\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      0.96      0.94      1405\n",
      "        True       0.93      0.88      0.91       959\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      2364\n",
      "   macro avg       0.93      0.92      0.92      2364\n",
      "weighted avg       0.93      0.93      0.93      2364\n",
      "\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      0.85      0.86       492\n",
      "        True       0.69      0.71      0.70       228\n",
      "\n",
      "   micro avg       0.81      0.81      0.81       720\n",
      "   macro avg       0.78      0.78      0.78       720\n",
      "weighted avg       0.81      0.81      0.81       720\n",
      "\n",
      "----------------------------------------------------\n",
      "trumpet\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.97      0.97      1303\n",
      "        True       0.96      0.95      0.95       858\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      2161\n",
      "   macro avg       0.96      0.96      0.96      2161\n",
      "weighted avg       0.96      0.96      0.96      2161\n",
      "\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.79      0.87      0.83       467\n",
      "        True       0.78      0.66      0.71       318\n",
      "\n",
      "   micro avg       0.78      0.78      0.78       785\n",
      "   macro avg       0.78      0.76      0.77       785\n",
      "weighted avg       0.78      0.78      0.78       785\n",
      "\n",
      "----------------------------------------------------\n",
      "ukulele\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.98      0.97      1279\n",
      "        True       0.95      0.92      0.94       563\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1842\n",
      "   macro avg       0.96      0.95      0.95      1842\n",
      "weighted avg       0.96      0.96      0.96      1842\n",
      "\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.82      0.91      0.87       408\n",
      "        True       0.74      0.56      0.64       182\n",
      "\n",
      "   micro avg       0.81      0.81      0.81       590\n",
      "   macro avg       0.78      0.74      0.75       590\n",
      "weighted avg       0.80      0.81      0.80       590\n",
      "\n",
      "----------------------------------------------------\n",
      "violin\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.85      0.92       623\n",
      "        True       0.90      1.00      0.95       818\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1441\n",
      "   macro avg       0.95      0.92      0.93      1441\n",
      "weighted avg       0.94      0.93      0.93      1441\n",
      "\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.88      0.71      0.79       237\n",
      "        True       0.84      0.94      0.89       394\n",
      "\n",
      "   micro avg       0.86      0.86      0.86       631\n",
      "   macro avg       0.86      0.83      0.84       631\n",
      "weighted avg       0.86      0.86      0.85       631\n",
      "\n",
      "----------------------------------------------------\n",
      "voice\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.88      0.94       426\n",
      "        True       0.95      1.00      0.97       950\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1376\n",
      "   macro avg       0.97      0.94      0.96      1376\n",
      "weighted avg       0.96      0.96      0.96      1376\n",
      "\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.89      0.93       150\n",
      "        True       0.93      0.98      0.95       224\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       374\n",
      "   macro avg       0.95      0.93      0.94       374\n",
      "weighted avg       0.95      0.94      0.94       374\n",
      "\n",
      "mAP: 0.6816869527724714\n"
     ]
    }
   ],
   "source": [
    "#%% Openmic baseline\n",
    "OMIC_DATA_NAME='NEW_OpenMIC.npz'\n",
    "OPENMIC = np.load(os.path.join(DATA_ROOT,'OpenMIC',OMIC_DATA_NAME))\n",
    "X_om_final, Y_true_om_final, Y_mask_om_final, sample_key_om_final = OPENMIC['X'], OPENMIC['Y_true'], OPENMIC['Y_mask'], OPENMIC['sample_key']\n",
    "\n",
    "split_train = pd.read_csv(os.path.join(DATA_ROOT, 'OpenMIC','split01_train.csv'), \n",
    "                              header=None, squeeze=True)\n",
    "split_test = pd.read_csv(os.path.join(DATA_ROOT, 'OpenMIC','split01_test.csv'), \n",
    "                     header=None, squeeze=True)\n",
    "\n",
    "train_set = set(split_train)\n",
    "test_set = set(split_test)\n",
    "\n",
    "idx_train, idx_test = [], []\n",
    "\n",
    "for idx, n in enumerate(sample_key_om_final):\n",
    "    if n in train_set:\n",
    "        idx_train.append(idx)\n",
    "    elif n in test_set:\n",
    "        idx_test.append(idx)\n",
    "\n",
    "idx_train = np.asarray(idx_train)\n",
    "idx_test = np.asarray(idx_test)\n",
    "\n",
    "X_train = X_om_final[idx_train]\n",
    "X_test = X_om_final[idx_test]\n",
    "\n",
    "\n",
    "Y_true_train = Y_true_om_final[idx_train]\n",
    "Y_true_test = Y_true_om_final[idx_test]\n",
    "\n",
    "Y_mask_train = Y_mask_om_final[idx_train]\n",
    "Y_mask_test = Y_mask_om_final[idx_test]\n",
    "\n",
    "\n",
    "\n",
    "# This dictionary will include the classifiers for each model\n",
    "models = dict()\n",
    "AP_TOT=0\n",
    "APS=[]\n",
    "# We'll iterate over all istrument classes, and fit a model for each one\n",
    "# After training, we'll print a classification report for each instrument\n",
    "for inst_num in range(20):\n",
    "    \n",
    "    # Map the instrument name to its column number\n",
    "        \n",
    "    # Step 1: sub-sample the data\n",
    "    \n",
    "    # First, we need to select down to the data for which we have annotations\n",
    "    # This is what the mask arrays are for\n",
    "    train_inst = Y_mask_train[:, inst_num]\n",
    "    test_inst = Y_mask_test[:, inst_num]\n",
    "    \n",
    "    # Here, we're using the Y_mask_train array to slice out only the training examples\n",
    "    # for which we have annotations for the given class\n",
    "    X_train_inst = X_train[train_inst]\n",
    "    \n",
    "    # Step 3: simplify the data by averaging over time\n",
    "    \n",
    "    # Let's arrange the data for a sklearn Random Forest model \n",
    "    # Instead of having time-varying features, we'll summarize each track by its mean feature vector over time\n",
    "    X_train_inst_sklearn = np.mean(X_train_inst, axis=1)\n",
    "    \n",
    "    # Again, we slice the labels to the annotated examples\n",
    "    # We thresold the label likelihoods at 0.5 to get binary labels\n",
    "    Y_true_train_inst = Y_true_train[train_inst, inst_num] >= 0.5\n",
    "\n",
    "    \n",
    "    # Repeat the above slicing and dicing but for the test set\n",
    "    X_test_inst = X_test[test_inst]\n",
    "    X_test_inst_sklearn = np.mean(X_test_inst, axis=1)\n",
    "    Y_true_test_inst = Y_true_test[test_inst, inst_num] >= 0.5\n",
    "\n",
    "    # Step 3.\n",
    "    # Initialize a new classifier\n",
    "    clf = RandomForestClassifier(max_depth=8, n_estimators=100, random_state=0)\n",
    "    \n",
    "    # Step 4.\n",
    "    clf.fit(X_train_inst_sklearn, Y_true_train_inst)\n",
    "\n",
    "    # Step 5.\n",
    "    # Finally, we'll evaluate the model on both train and test\n",
    "    Y_pred_train = clf.predict(X_train_inst_sklearn)\n",
    "    Y_pred_test = clf.predict(X_test_inst_sklearn)\n",
    "    \n",
    "    AP=average_precision_score(Y_true_test_inst, Y_pred_test, average=None)\n",
    "    APS.append(AP)\n",
    "    AP_TOT=AP_TOT+AP\n",
    "    \n",
    "    print('-' * 52)\n",
    "    print(classes[inst_num])\n",
    "    print('\\tTRAIN')\n",
    "    print(classification_report(Y_true_train_inst, Y_pred_train))\n",
    "    print('\\tTEST')\n",
    "    print(classification_report(Y_true_test_inst, Y_pred_test))\n",
    "    \n",
    "    # Store the classifier in our dictionary\n",
    "    models[classes[inst_num]] = clf\n",
    "\n",
    "print('mAP: {}'.format(AP_TOT/20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
